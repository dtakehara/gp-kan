\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}
\usepackage[title]{appendix}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{cancel}
\usepackage{hyperref}
\usepackage{amsthm}
    \newtheorem{definition}{Definition}[section]
    \newtheorem{theorem}{Theorem}
    \newtheorem{proposition}{Proposition}
\usepackage[
    backend=biber,
    style=numeric,
    citestyle=authoryear,
    sorting=none
]{biblatex}
\addbibresource{references.bib}
\geometry{a4paper, portrait, margin=2cm}

\title{Probabilistic Kolmogorovâ€“Arnold Networks}
\author{Andrew Siyuan Chen \\
        Engineering, Cambridge University \\
        sc2178@cantab.ac.uk}

\begin{document}
\maketitle

\section{Introduction}

Paper [\cite{KAN}] introduced the idea of using non-linear activation functions to replace traditional linear weight activation for the neurons in a Multi-Layer Perceptron (MLP), creating a Kolmogorov-Arnold Network (KAN). The results are significant, with the model possessing better fitting abilities and being 100 times more parameter efficient.

They used learnable B-Splines as support for the non-linear neurons. Here we extend that idea, replacing the B-Splines with 1-Dimensional Gaussian Processes [\cite{Gaussian-Processes-for-Machine-Learning}] to create probabilistic non-linear neurons, creating a Probabilistic Kolmogorov-Arnold Network (PKAN).

\section{Probabilistic Functions}
\subsection{Gaussian Process}
[\cite[p.13]{Gaussian-Processes-for-Machine-Learning}] defines a Gaussian Process (GP) as:
\begin{definition}
    \label{def:GP}
    A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}
\begin{equation}
    \begin{aligned}
        f                                    & \sim\mathcal{GP}(m,k)
        \\
        \text{where} \quad m(\boldsymbol{x}) & =\mathbb{E} [f(x)]
        \\
        k(\boldsymbol{x},\boldsymbol{x}')    & =\text{Covar}[f(x), f(x')]
    \end{aligned}
    \label{eq:GP_def}
\end{equation}
The mean of $f(x)$ is defined by mean function $m(x)$, and the covariance of $f(x)$ and $f(x')$ is defined by covariance function $k(x,x')$. Given a vector of known function values $\boldsymbol{h}$ and corresponding input space locations $\boldsymbol{z}$, and a new input location $x$, the posterior probability distribution of the function output $f(x)$ is:
\begin{equation}
    \begin{aligned}
        p(f(x)|\boldsymbol{h})     & =\mathcal{N}(f(x)|\mu, \Sigma)           \\
        \text{where}\quad\quad\mu & =m(x) + \boldsymbol{k}_{xh}K_{hh}^{-1}\boldsymbol{h} \\
        \Sigma                    & =k(x, x)-\boldsymbol{k}_{xh}K_{hh}^{-1}\boldsymbol{k}_{hx}        \\
        \boldsymbol{k}_{xh}^T                  & =\boldsymbol{k}_{hx}=\begin{bmatrix}
                                                k(x, z_1) \\
                                                k(x, z_2) \\
                                                \vdots
                                            \end{bmatrix}                  \\
        K_{hh}                    & =\begin{bmatrix}
                                         k(z_1,z_1) & k(z_1, z_2) & \hdots \\
                                         k(z_2,z_1) &             &        \\
                                         \vdots     &             &
                                     \end{bmatrix}
    \end{aligned}
    \label{eq:GP_pred}
\end{equation}

\subsection{Gaussian Process with a Gaussian Input}
GP is great for mapping a given input location $x$ to a function output distribution $\Tilde{f_x}\sim p(\Tilde{f_x})$. In its basic form however, the input location $x$ is deterministic. If we wish to use GP as non-linear activations in a deep multi-layer PKAN, we need a way to handle $\Tilde{x}\sim p(x)$. Below proposes a way of doing so.

First define the mean function to be linear:
\begin{equation}
    m(x) = ax+b
    \label{eq:mean_linear}
\end{equation}
and the covariance function to be the squared exponential function [\cite[p.83]{Gaussian-Processes-for-Machine-Learning}], which can be rewritten in the form of a Gaussian Distribution:
\begin{equation}
    k(x,x')=s^2\exp\left(-\frac{(x-x')^2}{2l^2}\right)=s^2l\sqrt{2\pi}\mathcal{N}(x|x',l^2)
    \label{eq:covar_se}
\end{equation}

We also restrict the input distribution to be Gaussian as well, giving:
\begin{equation}
    f\sim\mathcal{GP}\left(m(\cdot), k(\cdot, \cdot)\right),\quad \Tilde{x}\sim p(x)=\mathcal{N}(x|\mu_x,\sigma_x^2)
\end{equation}

Defining the output $\Tilde{y}$:
\begin{equation}
    \Tilde{y}=\int f(x)p(x)dx
\end{equation}
Here $p(x)$ is treated as a weight function that applies a scaling to the GP across the input space. Since each $f_x$ is a Gaussian random variable and integration is a linear operation on $f$, $\tilde{y}$ will be a Gaussian random variable as well.

Suppose that we have some known data $\boldsymbol{h}$, corresponding to locations $\boldsymbol{z}$, for the GP. Then:
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[\Tilde{y}|\boldsymbol{h}\right]&=\sqrt{2\pi}s^2l\boldsymbol{q}_{xh}K_{hh}^{-1}\boldsymbol{h} \\
        \text{Var}\left[\Tilde{y}|\boldsymbol{h}\right]&=\frac{s^2l}{\sqrt{ l^2+2\sigma_x^2}}-2\pi s^4 l^2 \boldsymbol{q}_{xh}K_{hh}^{-1}\boldsymbol{q}_{hx} \\
        \text{where }\boldsymbol{q}_{xh}&=\boldsymbol{q}_{hx}^T=\begin{bmatrix}
            \mathcal{N}(\mu_x|z_1,\sigma_x^2+l^2) \\
            \mathcal{N}(\mu_x|z_2,\sigma_x^2+l^2) \\
            \vdots
        \end{bmatrix}^T
    \label{eq:output_gaussian}
    \end{aligned}
\end{equation}


In practice, this corresponds to, for $N\rightarrow\infty$
\begin{equation}
    y=\text{mean}(\boldsymbol{y}),\quad \boldsymbol{y}\sim p(\boldsymbol{f}|\boldsymbol{x},\boldsymbol{h})=\mathcal{N}(\boldsymbol{f}|\mu, \Sigma),
    \quad \boldsymbol{x}=\begin{bmatrix}
        x_1 \\ x_2 \\ \vdots \\ x_N
    \end{bmatrix},\quad x_i\sim p(x)
\end{equation}
Then the mean can be calculated 


\newpage
\printbibliography
\newpage
\begin{appendices}
    \section{Proofs \& Formulas}
    \subsection{Integrating product of Gaussians}
    \begin{equation}
        \int\mathcal{N}(\boldsymbol{y}|W\boldsymbol{x}+\boldsymbol{b},\Sigma_2)\mathcal{N}(\boldsymbol{x}|\mu,\Sigma_1)d\boldsymbol{x}=\mathcal{N}(\boldsymbol{y}|W\mu+\boldsymbol{b},W\Sigma_1W^T+\Sigma_2)
        \label{eq:int_gaussian}
    \end{equation}
    Equation \ref{eq:int_gaussian} can be proven by considering Gaussian random variable $\Tilde{\boldsymbol{x}}\sim\mathcal{N}(\boldsymbol{x}|\mu,\Sigma_1)$ and $\Tilde{\boldsymbol{e}}\sim\mathcal{N}(\boldsymbol{e}|\boldsymbol{b},\Sigma_2)$. Then the Gaussian random variable $\Tilde{\boldsymbol{y}}=(W\Tilde{\boldsymbol{x}}+\Tilde{\boldsymbol{e}})\sim\mathcal{N}(\boldsymbol{y}|W\mu+\boldsymbol{b},W\Sigma_1W^T+\Sigma_2)$. Marginalizing the distribution for $\Tilde{\boldsymbol{y}}$ gives $p(\boldsymbol{y})=\int p(\boldsymbol{y}|\boldsymbol{x})p(\boldsymbol{x})d\boldsymbol{x}$, where we note that $p(\boldsymbol{x})=\mathcal{N}(\boldsymbol{x}|\mu,\Sigma_1)$ and $p(\boldsymbol{y}|\boldsymbol{x})=\mathcal{N}(\boldsymbol{y}|W\boldsymbol{x}+\boldsymbol{b},\Sigma_2)$, thus proving Equation \ref{eq:int_gaussian}.

    \subsection{Proof for Equation \ref{eq:output_gaussian}}
    Given 
    \begin{equation}
        \begin{aligned}
            m(x)&=ax+b \\
            k(x,x')&=s^2 \exp(-\frac{(x-x')^2}{2l^2})=\sqrt{2\pi}s^2l\mathcal{N}(x|x',l^2) \\
            p(x)&=\mathcal{N}(x|\mu_x,\sigma_x^2)
        \end{aligned}
    \end{equation}
    And defining 
    \begin{equation}
        \boldsymbol{q}_{xh}=\boldsymbol{q}_{hx}^T=\begin{bmatrix}
            \mathcal{N}(\mu_x|z_1,\sigma_x^2+l^2) \\
            \mathcal{N}(\mu_x|z_2,\sigma_x^2+l^2) \\
            \vdots
        \end{bmatrix}^T
    \end{equation}
    Applying Equation \ref{eq:int_gaussian}, we have the mean
    \begin{equation}
        \begin{aligned}
            \mathbb{E}\left[\Tilde{y}|\boldsymbol{h}\right]&=\int p(x)\mathbb{E}\left[f(x)|\boldsymbol{h}\right]dx \\
            &=\int p(x)\left(m(x)+\boldsymbol{k}_{xh}K_{hh}^{-1}\boldsymbol{h}\right)dx \\
            &=a\int xp(x)dx + b + \begin{bmatrix}
                \int p(x)k(x,z_1)dx \\
                \int p(x)k(x,z_2)dx \\
                \vdots
            \end{bmatrix}^T K_{hh}^{-1}\boldsymbol{h} \\
            &=a\mu_x+b+\sqrt{2\pi}s^2l\begin{bmatrix}
                \mathcal{N}(\mu_x|z_1,\sigma_x^2+l^2) \\
                \mathcal{N}(\mu_x|z_2,\sigma_x^2+l^2) \\
                \vdots
            \end{bmatrix}^TK_{hh}^{-1}\boldsymbol{h} \\
            &=a\mu_x+b+\sqrt{2\pi}s^2l\boldsymbol{q}_{xh}K_{hh}^{-1}\boldsymbol{h}
        \end{aligned}
    \end{equation}
    and the variance
    \begin{equation}
        \begin{aligned}
            \text{Var}\left[\Tilde{y}|\boldsymbol{h}\right]&=\int\int p(x)\text{Covar}\left[f(x),f(x')|\boldsymbol{h}\right]p(x')dxdx' \\
            &=\int\int p(x)k(x,x')p(x')dxdx'-\int\int p(x)\boldsymbol{k}_{xh}K_{hh}^{-1}\boldsymbol{k}_{hx'}p(x')dxdx'
        \label{eq:incomplete_var}
        \end{aligned}
    \end{equation}
    For the first term 
    \begin{equation}
        \begin{aligned}
            \int\int p(x)k(x,x')p(x')dxdx'&=\int\left(\int p(x)k(x,x')dx\right)p(x')dx \\
            &=\int \sqrt{2\pi}s^2l\mathcal{N}(\mu_x|x',l^2 + \sigma_x^2)p(x')dx' \\
            &=\sqrt{2\pi}s^2l\mathcal{N}(\mu_x|\mu_x,l^2+2\sigma_x^2) \\
            &=\frac{s^2l}{\sqrt{l^2+2\sigma_x^2}}
        \end{aligned}
    \end{equation}
    For the second term 
    \begin{equation}
        \begin{aligned}
            \int\int p(x)\boldsymbol{k}_{xh}K_{hh}^{-1}\boldsymbol{k}_{hx'}p(x')dxdx' 
            &=\int\left(\int p(x)\boldsymbol{k}_{xh}dx\right)K_{hh}^{-1}\boldsymbol{k}_{hx'}p(x')dx' \\
            &=\int \sqrt{2\pi}s^2l\boldsymbol{q}_{xh}K_{hh}^{-1}\boldsymbol{k}_{hx'}p(x')dx' \\
            &=\sqrt{2\pi}s^2l\boldsymbol{q}_{xh}K_{hh}^{-1} \int \boldsymbol{k}_{hx'}p(x')dx' \\
            &=2\pi s^4 l^2 \boldsymbol{q}_{xh}K_{hh}^{-1}\boldsymbol{q}_{hx}
        \end{aligned}
    \end{equation}
    Giving the variance in Equation \ref{eq:incomplete_var} to be:
    \begin{equation}
        \text{Var}\left[\Tilde{y}|\boldsymbol{h}\right]=\frac{s^2l}{\sqrt{l^2+2\sigma_x^2}} - 2\pi s^4 l^2 \boldsymbol{q}_{xh}K_{hh}^{-1}\boldsymbol{q}_{hx}
    \end{equation}
    As a sanity check, we can see that when $\sigma_x^2=0$ which indicates that the input $x$ is deterministic, the expressions return to the original GP posterior in Equation \ref{eq:GP_pred}
\end{appendices}
\end{document}